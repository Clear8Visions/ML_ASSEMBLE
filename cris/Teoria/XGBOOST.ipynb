{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosted regression trees  \n",
    "Metodo di ensemble che combina più alberi decisionali per creare un modello più potente, simile al **Random Forest**.  \n",
    "Questi modelli possono essere utilizzati *per la regressione e la classificazione*.  \n",
    "\n",
    "In contrasto con l'approccio della foresta casuale, l'aumento del gradiente funziona costruendo alberi in un gestore seriale, in cui ogni albero cerca di correggere gli errori del precedente.  \n",
    "\n",
    "Per impostazione predefinita, non esiste una randomizzazione negli alberi di regressione con gradiente boosted; Invece, viene utilizzata una forte pre-potatura. Gli alberi con gradiente boosting utilizzano spesso alberi molto poco profondi, con profondità da uno a cinque, il che rende il modello più piccolo in termini di memoria e rende le previsioni più veloci.  \n",
    "\n",
    "**L'idea** principale alla base del gradient boosting **è** quella di **combinare molti modelli semplici** (in questo contesto noti come *weak-learner*), come alberi poco profondi.  \n",
    "\n",
    "Ogni albero può fornire solo previsioni valide su una parte dei dati, quindi vengono aggiunti sempre più alberi per migliorare iterativamente le prestazioni.  \n",
    "\n",
    "**Sono generalmente un po' più sensibili alle impostazioni dei parametri** rispetto alle foreste casuali, ma possono fornire una migliore precisione se i parametri sono impostati correttamente \n",
    "\n",
    "Oltre alla **pre-potatura** e al **numero di alberi** nell'insieme (*n_estimators*), un altro *parametro importante dell'aumento del gradiente è il **learning_rate***, che controlla con quanta forza ogni albero cerca di correggere gli errori degli alberi precedenti.  \n",
    "Un tasso di apprendimento più elevato significa che ogni albero può apportare correzioni più forti, consentendo modelli più complessi.  \n",
    "\n",
    "L'aggiunta di più alberi all'insieme aumenta anche la complessità del modello, poiché il modello ha più possibilità di correggere gli errori nel set di addestramento. \n",
    "\n",
    "Di seguito è riportato un esempio di utilizzo di GradientBoostingClassifier nel set di dati sul cancro al seno. Per impostazione predefinita, vengono utilizzati 100 alberi con profondità massima 3 e una velocità di apprendimento di 0,1:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 1.000\n",
      "Accuracy on test set: 0.965\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, random_state=0)\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poiché la precisione del set di addestramento è del 100%, è probabile che si verifichi un overfitting.  \n",
    "\n",
    "Per ridurre l'overfit, potremmo applicare una pre-potatura più forte limitando la profondità massima o abbassare il tasso di apprendimento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training set: 0.972\n",
      "Accuracy on test set: 0.965\n"
     ]
    }
   ],
   "source": [
    "import mglearn\n",
    "import mglearn.plot_cross_validation\n",
    "gbrt = GradientBoostingClassifier(random_state=1, max_depth=1, learning_rate=0.04)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possiamo vedere che l'importanza delle caratteristiche degli alberi potenziati dal gradiente è in qualche modo simile all'importanza delle caratteristiche delle foreste casuali, anche se l'aumento del gradiente ha completamente ignorato alcune delle caratteristiche.  \n",
    "\n",
    "Poiché sia l'aumento del gradiente che le foreste casuali si comportano bene su tipi di dati simili, un approccio comune consiste nel provare prima le foreste casuali, che funzionano in modo abbastanza robusto.  \n",
    "\n",
    "Se le foreste random funzionano bene ma il tempo di previsione è prezioso, o è importante spremere l'ultima percentuale di accuratezza dal modello di apprendimento automatico, il passaggio al gradient boosting spesso aiuta.  \n",
    "\n",
    "Se si desidera applicare il gradient boosting a un problema su larga scala, potrebbe valere la pena esaminare il pacchetto xgboost e la sua interfaccia Python, che al momento della scrittura è più veloce (e talvolta più facile da regolare) rispetto all'implementazione scikit-learn del gradient boosting su molti set di dati."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punti di forza, di debolezza e parametri  \n",
    "Gli CGBoost Trees sono tra i modelli più potenti e ampiamente utilizzati per l'apprendimento supervisionato.  \n",
    "\n",
    "Il loro principale svantaggio è che richiedono un'attenta messa a punto dei parametri e possono richiedere molto tempo per l'addestramento.  \n",
    "Analogamente ad altri modelli basati su alberi, l'algoritmo funziona bene senza ridimensionamento e su una combinazione di funzionalità binarie e continue. **Come con altri modelli basati su alberi, spesso non funziona bene su dati sparsi ad alta dimensione**(ossia dataset in cui la maggior parte delle caratteristiche ha valori zero). I parametri principali dei modelli di albero con gradiente potenziato sono il numero di alberi, i n_esti e il learning_rate, che controlla il grado in cui ogni albero è autorizzato a correggere gli errori degli alberi precedenti."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
